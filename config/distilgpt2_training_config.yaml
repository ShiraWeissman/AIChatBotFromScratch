language_modeling:
  dataset_type: "language_modeling"  # "language_modeling" for TinyStories, "question_answering" for FairytaleQA
  dataset_name: "tinystories"
  data_path: "data/processed"
  checkpoint_dir: "models/distilgpt2_LM_model/checkpoints"
  pretrained_model_path: null
  pretrained_model_name: "distilgpt2"
  pretrained_tokenizer_path: null
  pretrained_tokenizer: "distilgpt2"
  model_type: "distilgpt2"
  report_to: null

  training:
    batch_size: 16
    epochs: 3
    learning_rate: 5e-5
    evaluation_strategy: "epoch"
    save_strategy: "epoch"
    save_total_limit: 2
    logging_steps: 100
  device: "cuda"  # "cuda" for GPU, "cpu" for CPU
  save_path: "models/distilgpt2_LM_model"
question_answering:
  dataset_type: "question_answering"  # "language_modeling" for TinyStories, "question_answering" for FairytaleQA
  dataset_name: "fairytaleQA"
  data_path: "data/processed"
  checkpoint_dir: "models/distilbert_QA_model/checkpoints"
  pretrained_model_name: "train/models/distilbert_lm"
  model_type: "distilbert"
  report_to: null

  training:
    batch_size: 16
    epochs: 3
    learning_rate: 5e-5
  device: "cuda"  # "cuda" for GPU, "cpu" for CPU
